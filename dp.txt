# Importing the data
1. df = pd.read_csv('file.csv')

2. url = 'abc.com'
   df  = pd.read_csv(url)
----------------------------

# Data Cleaning
Check and remove null values

General info for all the columns
1. df.info()

Gives the sum of null values in all the columns
2. df.isna().sum()

Gives the True & False values for any null values within all the columns.
3. df.isna().all()

Drop the all columns with any null values.
4. df.dropna(axis=1, how='all', inplace=True)

---------------------------------------------

# Treating zeros(0) and empty strings within a column
Step 1. nan_values = float("Nan")
Step 2. df.replace(0,nan_values,inplace=True) 
        df.replace("",nan_values,inplace=True)
Step 3. df.dropna(axis=1, how='all', inplace=True)

------------------------------------------------

# Remove Rows With Empty Columns
df.drpna(inplace=True)

------------------------------------------------
Treating Duplicated
# count duplicate rows
1. df.duplicated().sum()

# Dropping duplicated
1. df.drop_duplicates

-----------------------------------------------
Filling Missing Values
# Filling with zero(0)
df.fillna(0)

# Filling in specific column
df['col'].fillna(0)

# Fill different columns with different values
Step 1. values = {"A":'value1',"B":'value2',"C":'value3'}
Step 2. df.fillna(value=values)

---------------------------------------------------

Removing/Replacing Special characters from column values
df.replace('$','INR',regex=True).astype(object)

---------------------------------------------------

Download File As .csv file
df.to_csv(r'path\filename.csv',index=False)

---------------------------------------------------
Removing One or Multiple Columns
# Dropping one column
df.drop(['Col1'],axis=1)

# Dropping more than one column
df.drop(['col1','col2'],axis=1)
---------------------------------------------------

Selecting df on condition on one column
df = df[df['col1']>3.5]
df_col1 = df[[col1]]
df_col1

--------------------------------------------------
Group a df and and sort on another column
new_df = df.sort_values(['col1'],ascending=False).groupby('col2')
new_df.haed()

--------------------------------------------------

Top 20 columns with highest value of other column
df.groupby('col1').sum('col2')['col2'].nlargest(20)

--------------------------------------------------

Removing specific cahracters from a column values
# Splitting at ','.
df['col1'] = df['col1'].apply(lambda x: x.split(,)[0])

-------------------------------------------------

Merging DFs
merged_df = df1.merge(df2,on='df2col1',how='inner')
merged_df

------------------------------------------------
Matplot lib
plt.pie(x,y)
plt.title('title')
plt.show()

Plotly Exprex
fig = px.pie(df,values = '',names='', title='')
fig.show()

------------------------------------------------

import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib
import os
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')

-------------------------------------
SQL CTE & WINDOWS FUNCTION
SELECT Name, Age, Department, Salary, 
 AVERAGE(Salary) OVER( PARTITION BY Department ORDER BY Age) AS Avg_Salary
 FROM employee

/* CTE - features overviews w.r.t. cmd status */
with overview (user_ID, country, currency, trade_size, contract_size, 
               buy_sell) as (
select 
    users.login_hash,
    users.country_hash,
    users.currency,
    sum(trades.volume) ,
    sum(trades.contractsize),
    trades.cmd
from trades
join users
on trades.login_hash = users.login_hash
group by users.country_hash,users.currency,users.login_hash,trades.cmd
order by sum(trades.volume) 
desc limit 10)
                    
select 
    user_ID,
    country,
    currency,
    trade_size,
    contract_size,
    buy_sell
from 
    overview
where
    buy_sell = 1;
    
/* Rank and put countries in row numbers according to net trade size */
select 
rank() over(partition by users.country_hash order by sum(volume) desc) 
as user_rank, country_hash, trades.login_hash,sum(volume) as net_trade_size, 
sum(contractsize) as contract_sum,
row_number() over(partition by users.country_hash order by sum(volume) desc) 
as user_row
from trades
join users
on trades.login_hash = users.login_hash
group by users.country_hash,trades.login_hash
order by sum(trades.volume) 
desc limit 10;
Footer


















